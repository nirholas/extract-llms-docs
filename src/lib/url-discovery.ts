/**
 * Intelligent URL Discovery System
 * 
 * A comprehensive, multi-strategy approach to finding llms.txt files.
 * Uses multiple discovery methods in parallel for maximum coverage:
 * 
 * 1. Direct URL patterns (subdomains, paths)
 * 2. robots.txt parsing
 * 3. Sitemap.xml analysis
 * 4. Homepage link scraping
 * 5. Meta tag analysis
 * 6. Well-known paths
 * 7. Common doc platform detection
 * 8. GitHub/GitLab repository checking
 * 9. OpenAPI/Swagger detection
 * 10. DNS-based subdomain discovery
 */

// ============================================================================
// Types & Interfaces
// ============================================================================

export interface DiscoveryResult {
  found: boolean
  url: string | null
  llmsTxtUrl: string | null
  type: 'llms-full' | 'llms-standard' | null
  scannedUrls: string[]
  timeElapsed: number
  discoveryMethod?: string
  confidence?: 'high' | 'medium' | 'low'
  additionalInfo?: {
    platform?: string
    sitemapFound?: boolean
    robotsTxtFound?: boolean
    githubRepo?: string
  }
}

export interface DiscoveryProgress {
  currentUrl: string
  scannedCount: number
  totalToScan: number
  status: 'scanning' | 'found' | 'not-found' | 'error'
  currentStrategy?: string
}

interface LlmsTxtCheckResult {
  found: boolean
  url: string | null
  type: 'llms-full' | 'llms-standard' | null
  content?: string
}

interface DiscoveredUrl {
  url: string
  priority: number  // Lower = higher priority
  source: string
}

// ============================================================================
// Configuration
// ============================================================================

const CONFIG = {
  // Timeout settings
  defaultTimeoutMs: 30000,
  perRequestTimeoutMs: 5000,
  
  // Batch processing
  batchSize: 8,
  maxUrlsToScan: 100,
  
  // Common documentation subdomains (ordered by likelihood)
  subdomains: [
    'docs',
    'api-docs',
    'documentation',
    'developers',
    'developer',
    'dev',
    'api',
    'reference',
    'help',
    'support',
    'learn',
    'guide',
    'guides',
    'wiki',
    'kb',
    'knowledge',
    'manual',
    'handbook',
    'resources',
    'devdocs',
    'apidocs',
    'dev-docs',
    'api-reference',
    'portal',
    'devportal',
    'dev-portal',
    'platform',
    'openapi',
    'swagger',
    'spec',
    'specs',
  ],
  
  // Common documentation paths (ordered by likelihood)
  docPaths: [
    '/docs',
    '/documentation',
    '/api',
    '/api-docs',
    '/developer',
    '/developers',
    '/reference',
    '/help',
    '/guide',
    '/guides',
    '/learn',
    '/manual',
    '/handbook',
    '/resources',
    '/wiki',
    '/kb',
    '/knowledge-base',
    '/support',
    '/getting-started',
    '/quickstart',
    '/tutorials',
    '/examples',
    '/api-reference',
    '/openapi',
    '/swagger',
  ],
  
  // Well-known paths to check
  wellKnownPaths: [
    '/.well-known/llms.txt',
    '/.well-known/llms-full.txt',
    '/llms.txt',
    '/llms-full.txt',
  ],
  
  // Common documentation platforms and their patterns
  docPlatforms: {
    gitbook: {
      indicators: ['gitbook.io', 'gitbook.com', 'x-gitbook'],
      llmsPaths: ['/llms.txt', '/llms-full.txt'],
    },
    readme: {
      indicators: ['readme.io', 'readme.com', 'x-readme'],
      llmsPaths: ['/llms.txt', '/llms-full.txt'],
    },
    mintlify: {
      indicators: ['mintlify.', 'x-mintlify'],
      llmsPaths: ['/llms.txt', '/llms-full.txt'],
    },
    docusaurus: {
      indicators: ['docusaurus', '__docusaurus'],
      llmsPaths: ['/llms.txt', '/docs/llms.txt'],
    },
    notion: {
      indicators: ['notion.site', 'notion.so'],
      llmsPaths: ['/llms.txt'],
    },
    confluence: {
      indicators: ['confluence', 'atlassian.net'],
      llmsPaths: ['/llms.txt'],
    },
    vitepress: {
      indicators: ['vitepress', '.vitepress'],
      llmsPaths: ['/llms.txt', '/guide/llms.txt'],
    },
    docsify: {
      indicators: ['docsify', 'window.$docsify'],
      llmsPaths: ['/llms.txt', '/#/llms.txt'],
    },
    mkdocs: {
      indicators: ['mkdocs', 'material for mkdocs'],
      llmsPaths: ['/llms.txt', '/llms-full.txt'],
    },
    sphinx: {
      indicators: ['sphinx', 'readthedocs'],
      llmsPaths: ['/llms.txt', '/_static/llms.txt'],
    },
    readthedocs: {
      indicators: ['readthedocs.io', 'readthedocs.org'],
      llmsPaths: ['/llms.txt', '/en/latest/llms.txt', '/en/stable/llms.txt'],
    },
  },
  
  // User agent for requests
  userAgent: 'llms-forge/1.0 (Intelligent Documentation Discovery; +https://llm.energy)',
}

// ============================================================================
// Utility Functions
// ============================================================================

/**
 * Parse a URL and extract domain components
 */
function parseUrl(inputUrl: string): {
  protocol: string
  hostname: string
  baseDomain: string
  tld: string
  fullDomain: string
  hasSubdomain: boolean
  subdomain: string | null
} | null {
  try {
    let normalizedUrl = inputUrl.trim()
    if (!normalizedUrl.startsWith('http://') && !normalizedUrl.startsWith('https://')) {
      normalizedUrl = `https://${normalizedUrl}`
    }
    
    const urlObj = new URL(normalizedUrl)
    const hostname = urlObj.hostname
    const protocol = urlObj.protocol
    const parts = hostname.split('.')
    
    let baseDomain: string
    let tld: string
    let subdomain: string | null = null
    
    // Handle common compound TLDs
    const compoundTlds = ['co.uk', 'com.au', 'co.nz', 'co.jp', 'com.br', 'co.in', 'org.uk', 'net.au']
    const lastTwo = parts.slice(-2).join('.')
    
    if (compoundTlds.includes(lastTwo) && parts.length >= 3) {
      tld = lastTwo
      baseDomain = parts[parts.length - 3]
      if (parts.length > 3) {
        subdomain = parts.slice(0, -3).join('.')
      }
    } else if (parts.length >= 2) {
      tld = parts[parts.length - 1]
      baseDomain = parts[parts.length - 2]
      if (parts.length > 2) {
        subdomain = parts.slice(0, -2).join('.')
      }
    } else {
      baseDomain = parts[0]
      tld = 'com'
    }
    
    const fullDomain = `${baseDomain}.${tld}`
    const hasSubdomain = subdomain !== null && subdomain !== 'www'
    
    return {
      protocol,
      hostname,
      baseDomain,
      tld,
      fullDomain,
      hasSubdomain,
      subdomain: subdomain === 'www' ? null : subdomain,
    }
  } catch {
    return null
  }
}

/**
 * Create a fetch request with timeout
 */
async function fetchWithTimeout(
  url: string,
  options: RequestInit = {},
  timeoutMs: number = CONFIG.perRequestTimeoutMs
): Promise<Response> {
  const controller = new AbortController()
  const timeoutId = setTimeout(() => controller.abort(), timeoutMs)
  
  try {
    const response = await fetch(url, {
      ...options,
      signal: controller.signal,
      headers: {
        'User-Agent': CONFIG.userAgent,
        'Accept': (options.headers as Record<string, string>)?.['Accept'] || '*/*',
        ...(options.headers as Record<string, string>),
      },
    })
    clearTimeout(timeoutId)
    return response
  } catch (error) {
    clearTimeout(timeoutId)
    throw error
  }
}

/**
 * Check if content looks like valid llms.txt
 */
function isValidLlmsTxtContent(content: string): boolean {
  if (!content || content.length < 10) return false
  
  const trimmed = content.trim().toLowerCase()
  
  // Reject HTML
  if (trimmed.startsWith('<!doctype') || trimmed.startsWith('<html') || trimmed.startsWith('<head')) {
    return false
  }
  
  // Should contain markdown-like content
  const hasMarkdownIndicators = 
    content.includes('#') ||  // Headers
    content.includes('>') ||  // Blockquotes
    content.includes('- ') || // Lists
    content.includes('* ') || // Lists
    content.includes('[') ||  // Links
    content.includes('```')   // Code blocks
  
  return hasMarkdownIndicators
}

// ============================================================================
// Core Discovery Functions
// ============================================================================

/**
 * Check if a specific URL has an llms.txt file
 */
async function checkForLlmsTxt(
  baseUrl: string,
  signal?: AbortSignal
): Promise<LlmsTxtCheckResult> {
  const filesToTry = ['llms-full.txt', 'llms.txt']
  const normalizedBase = baseUrl.replace(/\/$/, '')
  
  for (const file of filesToTry) {
    const testUrl = `${normalizedBase}/${file}`
    
    try {
      const response = await fetchWithTimeout(testUrl, {
        method: 'GET',
        headers: { 'Accept': 'text/plain, text/markdown, */*' },
        redirect: 'follow',
      })
      
      if (response.ok) {
        const text = await response.text()
        
        if (isValidLlmsTxtContent(text)) {
          return {
            found: true,
            url: testUrl,
            type: file === 'llms-full.txt' ? 'llms-full' : 'llms-standard',
            content: text.slice(0, 500), // Preview
          }
        }
      }
    } catch {
      // Continue to next file
    }
  }
  
  return { found: false, url: null, type: null }
}

/**
 * Check well-known paths for llms.txt
 */
async function checkWellKnownPaths(
  baseUrl: string,
  signal?: AbortSignal
): Promise<LlmsTxtCheckResult> {
  const parsed = parseUrl(baseUrl)
  if (!parsed) return { found: false, url: null, type: null }
  
  const baseUrls = [
    `${parsed.protocol}//${parsed.hostname}`,
    `${parsed.protocol}//${parsed.fullDomain}`,
    `${parsed.protocol}//www.${parsed.fullDomain}`,
  ]
  
  for (const base of Array.from(new Set(baseUrls))) {
    for (const path of CONFIG.wellKnownPaths) {
      const testUrl = `${base}${path}`
      
      try {
        const response = await fetchWithTimeout(testUrl, {
          method: 'GET',
          headers: { 'Accept': 'text/plain, text/markdown, */*' },
          redirect: 'follow',
        })
        
        if (response.ok) {
          const text = await response.text()
          if (isValidLlmsTxtContent(text)) {
            return {
              found: true,
              url: testUrl,
              type: path.includes('llms-full') ? 'llms-full' : 'llms-standard',
            }
          }
        }
      } catch {
        // Continue
      }
    }
  }
  
  return { found: false, url: null, type: null }
}

// ============================================================================
// Strategy 1: Generate URL Patterns
// ============================================================================

function generateUrlPatterns(inputUrl: string): DiscoveredUrl[] {
  const urls: DiscoveredUrl[] = []
  const parsed = parseUrl(inputUrl)
  
  if (!parsed) {
    return [{ url: inputUrl, priority: 0, source: 'input' }]
  }
  
  const { protocol, hostname, baseDomain, tld, fullDomain, hasSubdomain, subdomain } = parsed
  
  // If user provided a specific subdomain, prioritize it
  if (hasSubdomain && subdomain) {
    urls.push({ url: `${protocol}//${hostname}`, priority: 0, source: 'user-provided-subdomain' })
  }
  
  // Add subdomain variations
  CONFIG.subdomains.forEach((sub, index) => {
    urls.push({
      url: `${protocol}//${sub}.${fullDomain}`,
      priority: 10 + index,
      source: 'subdomain-pattern',
    })
  })
  
  // Add main domain
  urls.push({ url: `${protocol}//${fullDomain}`, priority: 5, source: 'main-domain' })
  urls.push({ url: `${protocol}//www.${fullDomain}`, priority: 6, source: 'www-domain' })
  
  // Add path-based documentation locations
  CONFIG.docPaths.forEach((path, index) => {
    urls.push({
      url: `${protocol}//${fullDomain}${path}`,
      priority: 50 + index,
      source: 'path-pattern',
    })
    urls.push({
      url: `${protocol}//www.${fullDomain}${path}`,
      priority: 51 + index,
      source: 'path-pattern',
    })
  })
  
  // Deduplicate and sort by priority
  const seen = new Set<string>()
  return urls
    .filter(u => {
      if (seen.has(u.url)) return false
      seen.add(u.url)
      return true
    })
    .sort((a, b) => a.priority - b.priority)
}

// ============================================================================
// Strategy 2: Parse robots.txt
// ============================================================================

async function parseRobotsTxt(inputUrl: string): Promise<DiscoveredUrl[]> {
  const urls: DiscoveredUrl[] = []
  const parsed = parseUrl(inputUrl)
  
  if (!parsed) return urls
  
  const robotsUrls = [
    `${parsed.protocol}//${parsed.hostname}/robots.txt`,
    `${parsed.protocol}//${parsed.fullDomain}/robots.txt`,
  ]
  
  for (const robotsUrl of Array.from(new Set(robotsUrls))) {
    try {
      const response = await fetchWithTimeout(robotsUrl, {
        headers: { 'Accept': 'text/plain' },
      })
      
      if (!response.ok) continue
      
      const text = await response.text()
      
      // Look for Sitemap directives
      const sitemapMatches = Array.from(text.matchAll(/Sitemap:\s*(\S+)/gi))
      for (const match of sitemapMatches) {
        urls.push({
          url: match[1].trim(),
          priority: 15,
          source: 'robots-sitemap',
        })
      }
      
      // Look for Allow/Disallow patterns that might indicate docs
      const lines = text.split('\n')
      for (const line of lines) {
        const allowMatch = line.match(/Allow:\s*(\S+)/i)
        if (allowMatch) {
          const path = allowMatch[1]
          if (/docs?|api|developer|reference|guide|help/i.test(path)) {
            const baseUrl = robotsUrl.replace('/robots.txt', '')
            urls.push({
              url: `${baseUrl}${path.replace(/\*/g, '')}`,
              priority: 30,
              source: 'robots-allow',
            })
          }
        }
      }
    } catch {
      // Continue to next URL
    }
  }
  
  return urls
}

// ============================================================================
// Strategy 3: Parse Sitemap
// ============================================================================

async function parseSitemap(sitemapUrl: string, depth: number = 0): Promise<DiscoveredUrl[]> {
  const urls: DiscoveredUrl[] = []
  
  // Prevent infinite recursion
  if (depth > 2) return urls
  
  try {
    const response = await fetchWithTimeout(sitemapUrl, {
      headers: { 'Accept': 'application/xml, text/xml, */*' },
    })
    
    if (!response.ok) return urls
    
    const text = await response.text()
    
    // Check if it's a sitemap index
    if (text.includes('<sitemapindex')) {
      const sitemapLocMatches = Array.from(text.matchAll(/<loc>([^<]+)<\/loc>/gi))
      for (const match of sitemapLocMatches) {
        const nestedUrl = match[1].trim()
        if (nestedUrl.includes('sitemap') && nestedUrl.endsWith('.xml')) {
          // Recursively parse nested sitemaps (limit depth)
          const nestedUrls = await parseSitemap(nestedUrl, depth + 1)
          urls.push(...nestedUrls)
        }
      }
    }
    
    // Extract URLs from sitemap
    const locMatches = Array.from(text.matchAll(/<loc>([^<]+)<\/loc>/gi))
    for (const match of locMatches) {
      const url = match[1].trim()
      
      // Look for documentation-related URLs
      if (/docs?|api|developer|reference|guide|help|getting-started|quickstart/i.test(url)) {
        // Extract base doc URL
        try {
          const urlObj = new URL(url)
          const pathParts = urlObj.pathname.split('/').filter(Boolean)
          
          // Get the docs root
          for (let i = 0; i < pathParts.length; i++) {
            if (/docs?|api|developer|reference|guide|help/i.test(pathParts[i])) {
              const docsRoot = `${urlObj.protocol}//${urlObj.host}/${pathParts.slice(0, i + 1).join('/')}`
              urls.push({
                url: docsRoot,
                priority: 20,
                source: 'sitemap-docs',
              })
              break
            }
          }
        } catch {
          // Invalid URL, skip
        }
      }
    }
  } catch {
    // Silently fail
  }
  
  return urls
}

// ============================================================================
// Strategy 4: Scrape Homepage for Links
// ============================================================================

async function scrapeHomepageLinks(inputUrl: string): Promise<DiscoveredUrl[]> {
  const urls: DiscoveredUrl[] = []
  const parsed = parseUrl(inputUrl)
  
  if (!parsed) return urls
  
  const pagesToScrape = [
    `${parsed.protocol}//${parsed.hostname}`,
    `${parsed.protocol}//${parsed.fullDomain}`,
  ]
  
  for (const pageUrl of Array.from(new Set(pagesToScrape))) {
    try {
      const response = await fetchWithTimeout(pageUrl, {
        headers: { 'Accept': 'text/html' },
      })
      
      if (!response.ok) continue
      
      const html = await response.text()
      const baseHost = new URL(pageUrl).hostname.replace('www.', '')
      
      // Extract all href attributes
      const hrefMatches = Array.from(html.matchAll(/href=["']([^"']+)["']/gi))
      
      for (const match of hrefMatches) {
        let url = match[1]
        
        // Skip non-http links
        if (url.startsWith('#') || url.startsWith('javascript:') || 
            url.startsWith('mailto:') || url.startsWith('tel:')) {
          continue
        }
        
        // Convert relative URLs
        if (url.startsWith('/')) {
          url = `${parsed.protocol}//${new URL(pageUrl).host}${url}`
        } else if (!url.startsWith('http')) {
          continue
        }
        
        // Check if URL looks like documentation
        try {
          const urlObj = new URL(url)
          const urlHost = urlObj.hostname.replace('www.', '')
          
          // Must be same domain or known doc platform
          const isRelated = 
            urlHost.includes(baseHost) ||
            urlHost.includes(parsed.baseDomain) ||
            /gitbook|readme|notion|mintlify|docusaurus|readthedocs/.test(urlHost)
          
          if (!isRelated) continue
          
          // Check if path or subdomain indicates docs
          const isDocUrl = 
            /docs?|api-docs?|documentation|developer|reference|guide|help|learn/.test(urlObj.hostname) ||
            /docs?|api|developer|reference|guide|help|getting-started/.test(urlObj.pathname)
          
          if (isDocUrl) {
            urls.push({
              url: url.replace(/\/$/, ''),
              priority: 25,
              source: 'homepage-link',
            })
          }
        } catch {
          continue
        }
      }
      
      // Also look for meta tags
      const canonicalMatch = html.match(/<link[^>]+rel=["']canonical["'][^>]+href=["']([^"']+)["']/i)
      if (canonicalMatch) {
        urls.push({
          url: canonicalMatch[1],
          priority: 5,
          source: 'canonical',
        })
      }
      
      // Look for documentation link in nav/header
      const docLinkPatterns = [
        /href=["']([^"']+)["'][^>]*>(?:[^<]*(?:docs?|documentation|api|developers?|reference|guide))/gi,
        /<a[^>]+href=["']([^"']+)["'][^>]*class=["'][^"']*(?:docs?|nav|menu)[^"']*["']/gi,
      ]
      
      for (const pattern of docLinkPatterns) {
        const matches = Array.from(html.matchAll(pattern))
        for (const m of matches) {
          let docUrl = m[1]
          if (docUrl.startsWith('/')) {
            docUrl = `${parsed.protocol}//${new URL(pageUrl).host}${docUrl}`
          }
          if (docUrl.startsWith('http')) {
            urls.push({
              url: docUrl.replace(/\/$/, ''),
              priority: 15,
              source: 'nav-link',
            })
          }
        }
      }
    } catch {
      continue
    }
  }
  
  return urls
}

// ============================================================================
// Strategy 5: Detect Documentation Platform
// ============================================================================

async function detectDocPlatform(inputUrl: string): Promise<{
  platform: string | null
  urls: DiscoveredUrl[]
}> {
  const urls: DiscoveredUrl[] = []
  const parsed = parseUrl(inputUrl)
  
  if (!parsed) return { platform: null, urls }
  
  const pagesToCheck = [
    `${parsed.protocol}//${parsed.hostname}`,
    `${parsed.protocol}//${parsed.fullDomain}`,
  ]
  
  for (const pageUrl of Array.from(new Set(pagesToCheck))) {
    try {
      const response = await fetchWithTimeout(pageUrl, {
        headers: { 'Accept': 'text/html' },
      })
      
      if (!response.ok) continue
      
      const html = await response.text().then(t => t.toLowerCase())
      const headers = Object.fromEntries(response.headers.entries())
      const headersStr = JSON.stringify(headers).toLowerCase()
      
      // Check each platform
      for (const [platform, config] of Object.entries(CONFIG.docPlatforms)) {
        const isMatch = config.indicators.some(indicator => 
          html.includes(indicator.toLowerCase()) || 
          headersStr.includes(indicator.toLowerCase())
        )
        
        if (isMatch) {
          // Add platform-specific paths
          for (const path of config.llmsPaths) {
            urls.push({
              url: `${pageUrl}${path}`,
              priority: 5,
              source: `platform-${platform}`,
            })
          }
          
          return { platform, urls }
        }
      }
    } catch {
      continue
    }
  }
  
  return { platform: null, urls }
}

// ============================================================================
// Strategy 6: Check GitHub/GitLab Repository
// ============================================================================

async function checkGitRepository(inputUrl: string): Promise<{
  repoUrl: string | null
  urls: DiscoveredUrl[]
}> {
  const urls: DiscoveredUrl[] = []
  const parsed = parseUrl(inputUrl)
  
  if (!parsed) return { repoUrl: null, urls }
  
  // Try to find GitHub/GitLab link on homepage
  try {
    const response = await fetchWithTimeout(`${parsed.protocol}//${parsed.hostname}`, {
      headers: { 'Accept': 'text/html' },
    })
    
    if (response.ok) {
      const html = await response.text()
      
      // Look for GitHub links
      const githubMatch = html.match(/href=["'](https:\/\/github\.com\/[^"'\/]+\/[^"'\/]+)/i)
      if (githubMatch) {
        const repoUrl = githubMatch[1]
        const rawBase = repoUrl.replace('github.com', 'raw.githubusercontent.com')
        
        // Check common doc locations in repo
        urls.push(
          { url: `${rawBase}/main/llms.txt`, priority: 10, source: 'github-main' },
          { url: `${rawBase}/master/llms.txt`, priority: 11, source: 'github-master' },
          { url: `${rawBase}/main/docs/llms.txt`, priority: 12, source: 'github-docs' },
          { url: `${rawBase}/master/docs/llms.txt`, priority: 13, source: 'github-docs' },
          { url: `${rawBase}/main/llms-full.txt`, priority: 14, source: 'github-main' },
          { url: `${rawBase}/master/llms-full.txt`, priority: 15, source: 'github-master' },
        )
        
        // Also check GitHub Pages
        const repoMatch = repoUrl.match(/github\.com\/([^\/]+)\/([^\/]+)/)
        if (repoMatch) {
          const [, owner, repo] = repoMatch
          if (owner && repo) {
            urls.push(
              { url: `https://${owner}.github.io/${repo}/llms.txt`, priority: 8, source: 'github-pages' },
              { url: `https://${owner}.github.io/${repo}/llms-full.txt`, priority: 9, source: 'github-pages' },
            )
          }
        }
        
        return { repoUrl, urls }
      }
      
      // Look for GitLab links
      const gitlabMatch = html.match(/href=["'](https:\/\/gitlab\.com\/[^"']+)/i)
      if (gitlabMatch) {
        const repoUrl = gitlabMatch[1]
        urls.push(
          { url: `${repoUrl}/-/raw/main/llms.txt`, priority: 10, source: 'gitlab-main' },
          { url: `${repoUrl}/-/raw/master/llms.txt`, priority: 11, source: 'gitlab-master' },
        )
        return { repoUrl, urls }
      }
    }
  } catch {
    // Continue
  }
  
  return { repoUrl: null, urls }
}

// ============================================================================
// Strategy 7: Check OpenAPI/Swagger
// ============================================================================

async function checkOpenApiEndpoints(inputUrl: string): Promise<DiscoveredUrl[]> {
  const urls: DiscoveredUrl[] = []
  const parsed = parseUrl(inputUrl)
  
  if (!parsed) return urls
  
  const openApiPaths = [
    '/openapi.json',
    '/openapi.yaml',
    '/swagger.json',
    '/swagger.yaml',
    '/api/openapi.json',
    '/api/swagger.json',
    '/v1/openapi.json',
    '/v2/openapi.json',
    '/v3/openapi.json',
    '/api-docs',
    '/api-docs/swagger.json',
  ]
  
  const bases = [
    `${parsed.protocol}//${parsed.hostname}`,
    `${parsed.protocol}//${parsed.fullDomain}`,
    `${parsed.protocol}//api.${parsed.fullDomain}`,
  ]
  
  for (const base of Array.from(new Set(bases))) {
    for (const path of openApiPaths) {
      try {
        const response = await fetchWithTimeout(`${base}${path}`, {
          method: 'HEAD',
        })
        
        if (response.ok) {
          // OpenAPI found - docs are likely nearby
          urls.push(
            { url: base, priority: 8, source: 'openapi-base' },
            { url: `${base}/docs`, priority: 9, source: 'openapi-docs' },
          )
          break
        }
      } catch {
        continue
      }
    }
  }
  
  return urls
}

// ============================================================================
// Main Discovery Function
// ============================================================================

export async function discoverLlmsTxt(
  inputUrl: string,
  options: {
    timeoutMs?: number
    onProgress?: (progress: DiscoveryProgress) => void
    strategies?: ('patterns' | 'robots' | 'sitemap' | 'homepage' | 'platform' | 'github' | 'openapi' | 'wellknown')[]
  } = {}
): Promise<DiscoveryResult> {
  const {
    timeoutMs = CONFIG.defaultTimeoutMs,
    onProgress,
    strategies = ['wellknown', 'patterns', 'platform', 'homepage', 'robots', 'sitemap', 'github', 'openapi'],
  } = options
  
  const startTime = Date.now()
  const scannedUrls: string[] = []
  const allDiscoveredUrls: DiscoveredUrl[] = []
  const additionalInfo: DiscoveryResult['additionalInfo'] = {}
  
  // Helper to check timeout
  const isTimedOut = () => Date.now() - startTime > timeoutMs
  
  // Helper to report progress
  const reportProgress = (currentUrl: string, strategy: string, status: DiscoveryProgress['status'] = 'scanning') => {
    if (onProgress) {
      onProgress({
        currentUrl,
        scannedCount: scannedUrls.length,
        totalToScan: Math.max(scannedUrls.length + 10, allDiscoveredUrls.length),
        status,
        currentStrategy: strategy,
      })
    }
  }
  
  try {
    // Phase 1: Quick check well-known paths first (highest chance of success)
    if (strategies.includes('wellknown') && !isTimedOut()) {
      reportProgress('Checking well-known paths...', 'wellknown')
      const result = await checkWellKnownPaths(inputUrl)
      if (result.found && result.url) {
        scannedUrls.push(result.url)
        reportProgress(result.url, 'wellknown', 'found')
        return {
          found: true,
          url: result.url.replace(/\/llms(-full)?\.txt$/, ''),
          llmsTxtUrl: result.url,
          type: result.type,
          scannedUrls,
          timeElapsed: Date.now() - startTime,
          discoveryMethod: 'well-known-path',
          confidence: 'high',
        }
      }
    }
    
    // Phase 2: Gather URLs from all strategies in parallel
    reportProgress('Gathering potential documentation URLs...', 'discovery')
    
    const discoveryPromises: Promise<DiscoveredUrl[]>[] = []
    
    if (strategies.includes('patterns')) {
      discoveryPromises.push(Promise.resolve(generateUrlPatterns(inputUrl)))
    }
    
    if (strategies.includes('robots')) {
      discoveryPromises.push(parseRobotsTxt(inputUrl).then(urls => {
        if (urls.length > 0) additionalInfo.robotsTxtFound = true
        return urls
      }))
    }
    
    if (strategies.includes('homepage')) {
      discoveryPromises.push(scrapeHomepageLinks(inputUrl))
    }
    
    if (strategies.includes('platform')) {
      discoveryPromises.push(detectDocPlatform(inputUrl).then(result => {
        if (result.platform) additionalInfo.platform = result.platform
        return result.urls
      }))
    }
    
    if (strategies.includes('github')) {
      discoveryPromises.push(checkGitRepository(inputUrl).then(result => {
        if (result.repoUrl) additionalInfo.githubRepo = result.repoUrl
        return result.urls
      }))
    }
    
    if (strategies.includes('openapi')) {
      discoveryPromises.push(checkOpenApiEndpoints(inputUrl))
    }
    
    // Wait for all discovery strategies
    const discoveryResults = await Promise.allSettled(discoveryPromises)
    
    for (const result of discoveryResults) {
      if (result.status === 'fulfilled') {
        allDiscoveredUrls.push(...result.value)
      }
    }
    
    // Phase 3: Parse sitemaps found in robots.txt
    if (strategies.includes('sitemap') && !isTimedOut()) {
      const sitemapUrls = allDiscoveredUrls
        .filter(u => u.source === 'robots-sitemap')
        .map(u => u.url)
      
      // Also try standard sitemap locations
      const parsed = parseUrl(inputUrl)
      if (parsed) {
        sitemapUrls.push(
          `${parsed.protocol}//${parsed.hostname}/sitemap.xml`,
          `${parsed.protocol}//${parsed.fullDomain}/sitemap.xml`,
        )
      }
      
      for (const sitemapUrl of Array.from(new Set(sitemapUrls)).slice(0, 3)) {
        if (isTimedOut()) break
        const sitemapDiscovered = await parseSitemap(sitemapUrl)
        if (sitemapDiscovered.length > 0) {
          additionalInfo.sitemapFound = true
          allDiscoveredUrls.push(...sitemapDiscovered)
        }
      }
    }
    
    // Deduplicate and sort by priority
    const seen = new Set<string>()
    const uniqueUrls = allDiscoveredUrls
      .filter(u => {
        const normalized = u.url.replace(/\/$/, '').toLowerCase()
        if (seen.has(normalized)) return false
        seen.add(normalized)
        return true
      })
      .sort((a, b) => a.priority - b.priority)
      .slice(0, CONFIG.maxUrlsToScan)
    
    // Phase 4: Check all discovered URLs for llms.txt
    reportProgress(`Checking ${uniqueUrls.length} potential locations...`, 'checking')
    
    for (let i = 0; i < uniqueUrls.length; i += CONFIG.batchSize) {
      if (isTimedOut()) break
      
      const batch = uniqueUrls.slice(i, i + CONFIG.batchSize)
      
      reportProgress(batch[0].url, batch[0].source)
      
      const results = await Promise.allSettled(
        batch.map(async ({ url, source }) => {
          scannedUrls.push(url)
          const result = await checkForLlmsTxt(url)
          return { url, source, result }
        })
      )
      
      for (const result of results) {
        if (result.status === 'fulfilled' && result.value.result.found) {
          const { url, source, result: checkResult } = result.value
          
          reportProgress(checkResult.url!, source, 'found')
          
          return {
            found: true,
            url,
            llmsTxtUrl: checkResult.url,
            type: checkResult.type,
            scannedUrls,
            timeElapsed: Date.now() - startTime,
            discoveryMethod: source,
            confidence: source.includes('user-provided') || source.includes('well-known') ? 'high' : 
                       source.includes('platform') || source.includes('github') ? 'high' : 'medium',
            additionalInfo,
          }
        }
      }
    }
    
    // Not found
    reportProgress('', 'complete', 'not-found')
    
    return {
      found: false,
      url: null,
      llmsTxtUrl: null,
      type: null,
      scannedUrls,
      timeElapsed: Date.now() - startTime,
      additionalInfo,
    }
    
  } catch (error) {
    reportProgress('', 'error', 'error')
    
    return {
      found: false,
      url: null,
      llmsTxtUrl: null,
      type: null,
      scannedUrls,
      timeElapsed: Date.now() - startTime,
    }
  }
}

// ============================================================================
// Quick Check Function
// ============================================================================

/**
 * Quick check if a specific URL has llms.txt (no discovery)
 */
export async function quickCheckLlmsTxt(url: string): Promise<DiscoveryResult> {
  const startTime = Date.now()
  const result = await checkForLlmsTxt(url)
  
  return {
    found: result.found,
    url: result.found ? url : null,
    llmsTxtUrl: result.url,
    type: result.type,
    scannedUrls: [url],
    timeElapsed: Date.now() - startTime,
    discoveryMethod: 'direct',
    confidence: 'high',
  }
}

// ============================================================================
// Batch Discovery Function
// ============================================================================

/**
 * Discover llms.txt for multiple URLs in parallel
 */
export async function batchDiscoverLlmsTxt(
  urls: string[],
  options: {
    concurrency?: number
    timeoutMs?: number
    onProgress?: (completed: number, total: number, current: string) => void
  } = {}
): Promise<Map<string, DiscoveryResult>> {
  const { concurrency = 3, timeoutMs = 30000, onProgress } = options
  const results = new Map<string, DiscoveryResult>()
  
  // Process in batches
  for (let i = 0; i < urls.length; i += concurrency) {
    const batch = urls.slice(i, i + concurrency)
    
    const batchResults = await Promise.allSettled(
      batch.map(url => discoverLlmsTxt(url, { timeoutMs }))
    )
    
    batch.forEach((url, index) => {
      const result = batchResults[index]
      if (result.status === 'fulfilled') {
        results.set(url, result.value)
      } else {
        results.set(url, {
          found: false,
          url: null,
          llmsTxtUrl: null,
          type: null,
          scannedUrls: [],
          timeElapsed: 0,
        })
      }
      
      if (onProgress) {
        onProgress(i + index + 1, urls.length, url)
      }
    })
  }
  
  return results
}
