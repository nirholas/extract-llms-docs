{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"llm.energy","text":"<p>Extract documentation for AI agents. Fetch llms.txt and install.md from any documentation site, organized into downloadable markdown ready for Claude, ChatGPT, and other AI assistants.</p> <p> Get Started  View on GitHub</p>"},{"location":"#features","title":"Features","text":"<ul> <li> <p> Smart Extraction</p> <p>Automatically fetches <code>llms.txt</code>, <code>llms-full.txt</code>, and <code>install.md</code> from any documentation site</p> </li> <li> <p> Organized Output</p> <p>Splits content into individual markdown files by section for easy navigation</p> </li> <li> <p> Agent-Ready Format</p> <p>Includes <code>AGENT-GUIDE.md</code> with instructions optimized for AI assistants</p> </li> <li> <p> One-Click Download</p> <p>Get individual files or everything bundled as a convenient ZIP archive</p> </li> <li> <p>:material-terminal:{ .lg .middle } install.md Support</p> <p>Detect and extract LLM-executable installation instructions automatically</p> </li> <li> <p> MCP Server</p> <p>Integrate with Claude Desktop, Cursor, and other MCP-compatible tools</p> </li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"Web AppMCP ServerAPI <ol> <li>Visit llm.energy</li> <li>Enter a documentation URL</li> <li>Download the extracted files</li> </ol> <p>Add to your MCP client configuration:</p> <pre><code>{\n  \"mcpServers\": {\n    \"llm-energy\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@llm-energy/mcp-server\"]\n    }\n  }\n}\n</code></pre> <p>See MCP Server Installation for detailed setup.</p> <pre><code>curl -X POST https://llm.energy/api/extract \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"url\": \"docs.anthropic.com\"}'\n</code></pre> <p>See API Reference for all endpoints.</p>"},{"location":"#supported-standards","title":"Supported Standards","text":"<p>llm.energy supports two complementary standards for LLM-friendly documentation:</p> Standard Description Learn More llms.txt Machine-readable documentation at <code>/llms.txt</code> llms.txt Standard install.md LLM-executable installation instructions install.md Standard"},{"location":"#links","title":"Links","text":"<ul> <li> <p> Documentation</p> <p>Overview \u00b7 Examples \u00b7 API Reference</p> </li> <li> <p> MCP Server</p> <p>Installation \u00b7 Available Tools</p> </li> <li> <p> Open Source</p> <p>GitHub Repository</p> </li> </ul>"},{"location":"api-reference/","title":"API Reference","text":"<p>REST API for llm.energy.</p>"},{"location":"api-reference/#base-url","title":"Base URL","text":"<pre><code>https://llm.energy/api\n</code></pre>"},{"location":"api-reference/#endpoints","title":"Endpoints","text":""},{"location":"api-reference/#extract-documentation","title":"Extract Documentation","text":"<pre><code>POST /extract\n</code></pre> <p>Request Body</p> <pre><code>{\n  \"url\": \"docs.anthropic.com\"\n}\n</code></pre> <p>Response</p> <pre><code>{\n  \"success\": true,\n  \"source\": \"llms-full.txt\",\n  \"documents\": [\n    {\n      \"name\": \"getting-started.md\",\n      \"title\": \"Getting Started\",\n      \"tokens\": 1250\n    }\n  ],\n  \"stats\": {\n    \"totalDocuments\": 12,\n    \"totalTokens\": 45000,\n    \"processingTimeMs\": 234\n  }\n}\n</code></pre>"},{"location":"api-reference/#fetch-raw-content","title":"Fetch Raw Content","text":"<pre><code>GET /fetch?url={url}&amp;full={boolean}\n</code></pre> <p>Parameters</p> Name Type Default Description <code>url</code> string required Documentation site URL <code>full</code> boolean <code>true</code> Fetch llms-full.txt <p>Response</p> <pre><code>{\n  \"success\": true,\n  \"content\": \"# Documentation\\n\\n...\",\n  \"source\": \"llms-full.txt\"\n}\n</code></pre>"},{"location":"api-reference/#error-responses","title":"Error Responses","text":"<pre><code>{\n  \"success\": false,\n  \"error\": \"No llms.txt found at the specified URL\"\n}\n</code></pre> Status Description 400 Invalid request parameters 404 No llms.txt found 500 Server error"},{"location":"api-reference/#validate-installmd","title":"Validate install.md","text":"<pre><code>POST /validate-install\n</code></pre> <p>Request Body</p> <pre><code>{\n  \"content\": \"# my-tool\\n\\nOBJECTIVE: Install the tool...\"\n}\n</code></pre> <p>Or validate from URL:</p> <pre><code>{\n  \"url\": \"https://example.com/install.md\"\n}\n</code></pre> <p>Response</p> <pre><code>{\n  \"valid\": true,\n  \"parsed\": {\n    \"productName\": \"my-tool\",\n    \"objective\": \"Install the tool\",\n    \"doneWhen\": \"Tool is working\",\n    \"todoItems\": [...],\n    \"steps\": [...]\n  }\n}\n</code></pre>"},{"location":"api-reference/#check-installmd-existence","title":"Check install.md Existence","text":"<pre><code>GET /validate-install?url={url}\n</code></pre> <p>Response</p> <pre><code>{\n  \"exists\": true,\n  \"url\": \"https://example.com/install.md\",\n  \"valid\": true,\n  \"productName\": \"my-tool\"\n}\n</code></pre>"},{"location":"examples/","title":"Examples","text":"<p>Real-world examples of using llm.energy.</p>"},{"location":"examples/#extracting-anthropic-docs","title":"Extracting Anthropic Docs","text":"<pre><code># Web App\nhttps://llm.energy\n\n# Enter URL\ndocs.anthropic.com\n</code></pre> <p>Result: 15+ markdown files covering Claude API, prompt engineering, and more.</p>"},{"location":"examples/#extracting-mcp-docs","title":"Extracting MCP Docs","text":"<pre><code># URL\nmodelcontextprotocol.io\n</code></pre> <p>Result: Complete MCP specification, server implementation guides, and client examples.</p>"},{"location":"examples/#extracting-stripe-docs","title":"Extracting Stripe Docs","text":"<pre><code># URL\ndocs.stripe.com\n</code></pre> <p>Result: API reference, integration guides, and webhook documentation.</p>"},{"location":"examples/#using-with-claude","title":"Using with Claude","text":"<ol> <li>Extract documentation using llm.energy</li> <li>Download the ZIP archive</li> <li>In Claude, start a new conversation</li> <li>Upload <code>llms-full.md</code> or individual files</li> <li>Ask questions about the documentation</li> </ol> <p>Example prompt:</p> <p>\"Based on the uploaded documentation, how do I implement streaming responses?\"</p>"},{"location":"examples/#using-with-chatgpt","title":"Using with ChatGPT","text":"<ol> <li>Extract documentation</li> <li>Download files</li> <li>Upload to ChatGPT (GPT-4 with file upload)</li> <li>Reference the docs in your prompts</li> </ol>"},{"location":"examples/#using-installmd","title":"Using install.md","text":""},{"location":"examples/#extracting-installmd","title":"Extracting install.md","text":"<p>When extracting from a site that has install.md, llm.energy will automatically detect and include it:</p> <pre><code># URL with install.md\npython.langchain.com\n</code></pre> <p>Result: Documentation files + <code>install.md</code> with installation instructions.</p>"},{"location":"examples/#creating-installmd","title":"Creating install.md","text":"<p>Use the install.md Generator to create your own:</p> <ol> <li>Go to the Generator page</li> <li>Fill in product name, objective, and done criteria</li> <li>Add TODO items (checklist for the LLM)</li> <li>Add installation steps with code blocks</li> <li>Download the generated <code>install.md</code></li> </ol>"},{"location":"examples/#using-installmd-with-ai-agents","title":"Using install.md with AI Agents","text":"<p>Give an AI agent the install.md file and ask it to install the software:</p> <p>Example prompt:</p> <p>\"Follow the instructions in install.md to set up this tool on my machine\"</p> <p>The agent will:</p> <ol> <li>Read the OBJECTIVE to understand the goal</li> <li>Execute each step in the TODO list</li> <li>Verify completion against DONE WHEN criteria</li> </ol>"},{"location":"examples/#using-the-mcp-server","title":"Using the MCP Server","text":"<p>With the MCP server, AI assistants can extract docs programmatically:</p> <p>Claude Desktop</p> <pre><code>You: \"Extract the Vercel documentation and tell me about edge functions\"\n\nClaude: [Uses extract_documentation tool]\n        [Reads relevant sections]\n        \"Based on the Vercel docs, edge functions are...\"\n</code></pre> <p>Example conversation:</p> <pre><code>You: \"What MCP tools do you have for documentation?\"\n\nClaude: \"I have access to llm-energy with these tools:\n        - extract_documentation: Extract docs from a URL\n        - fetch_llms_txt: Get raw llms.txt content\n        - get_document: Retrieve a cached document\n        - list_documents: List all cached docs\"\n\nYou: \"Extract docs from docs.anthropic.com\"\n\nClaude: [Extracts and summarizes the documentation]\n</code></pre>"},{"location":"examples/#supported-sites","title":"Supported Sites","text":"<p>Sites known to have llms.txt support:</p> Site URL Anthropic docs.anthropic.com MCP modelcontextprotocol.io Stripe docs.stripe.com Vercel vercel.com/docs Supabase supabase.com/docs Mintlify mintlify.com/docs <p>Check any site</p> <p>Try adding <code>/llms.txt</code> or <code>/llms-full.txt</code> to any documentation URL to see if it's supported.</p>"},{"location":"install-md-standard/","title":"The install.md Standard","text":"<p>Understanding the install.md standard for LLM-executable installation instructions.</p>"},{"location":"install-md-standard/#what-is-installmd","title":"What is install.md?","text":"<p>The install.md standard is a convention for providing installation instructions in a format designed for AI agents to execute autonomously.</p> <p>Similar to how <code>llms.txt</code> provides documentation context for LLMs, <code>install.md</code> provides structured installation instructions that LLMs can follow step-by-step.</p>"},{"location":"install-md-standard/#why-installmd","title":"Why install.md?","text":"<p>The Problem:</p> <p>Traditional installation documentation is written for humans. It uses prose, assumes context, and often requires interpretation. AI agents need explicit, step-by-step instructions with clear success criteria.</p> <p>The Solution:</p> <p>install.md provides:</p> <ul> <li>Clear objectives - What the installation should achieve</li> <li>Success criteria - How to verify installation worked</li> <li>TODO checklist - Steps the agent should complete</li> <li>Structured steps - Detailed instructions with code blocks</li> <li>Environment adaptation - The LLM adapts to the user's setup</li> </ul>"},{"location":"install-md-standard/#file-location","title":"File Location","text":"File Location <code>/install.md</code> Root of documentation or project <code>/docs/install.md</code> Documentation subdirectory"},{"location":"install-md-standard/#format","title":"Format","text":"<p>install.md uses a specific format with keywords that guide LLM execution:</p> <pre><code># product-name\n\n&gt; Description of the product\n\nI want you to install [Product] for me. Execute all the steps below autonomously.\n\nOBJECTIVE: What the installation should achieve.\n\nDONE WHEN: Specific verification criteria.\n\n## TODO\n\n- [ ] First step to complete\n- [ ] Second step to complete\n- [ ] Verify installation\n\n## Prerequisites\n\nInstructions for prerequisites...\n\n```bash\nnode --version\n</code></pre>"},{"location":"install-md-standard/#install","title":"Install","text":"<p>Installation instructions...</p> <pre><code>npm install -g product-name\n</code></pre>"},{"location":"install-md-standard/#verify","title":"Verify","text":"<p>Verification instructions...</p> <pre><code>product-name --version\n</code></pre> <p>EXECUTE NOW: Complete the above TODO list to achieve: [objective]. <pre><code>---\n\n## Required Elements\n\n| Element | Purpose |\n|---------|---------|\n| `# product-name` | H1 header with lowercase-hyphenated product name |\n| `OBJECTIVE:` | What the installation should achieve |\n| `DONE WHEN:` | Specific verification criteria |\n| `## TODO` | Markdown checkbox list of steps |\n| `EXECUTE NOW:` | Final call-to-action referencing TODO and objective |\n\n---\n\n## Optional Elements\n\n| Element | Purpose |\n|---------|---------|\n| `&gt; description` | Blockquote describing the product |\n| Action prompt | \"I want you to install...\" instruction |\n| Step sections | `## Prerequisites`, `## Install`, etc. |\n| Code blocks | Commands in fenced code blocks |\n| Platform variants | Multiple options for different environments |\n\n---\n\n## Usage\n\nUsers can pipe install.md directly to an AI agent:\n\n```bash\n# Using Claude\ncurl -fsSL https://docs.example.com/install.md | claude\n\n# Using other AI tools\ncurl -fsSL https://docs.example.com/install.md | ai-agent\n</code></pre></p> <p>Or paste the contents into any LLM conversation.</p>"},{"location":"install-md-standard/#relationship-to-llmstxt","title":"Relationship to llms.txt","text":"File Purpose <code>llms.txt</code> Helps LLMs understand your software <code>install.md</code> Tells LLMs how to install your software <p>Your install.md can link to llms.txt for troubleshooting, configuration details, or additional context during installation.</p>"},{"location":"install-md-standard/#best-practices","title":"Best Practices","text":"<p>1. Be Explicit</p> <p>Write for an agent that has no prior context. State exactly what needs to happen.</p> <p>2. Include Verification</p> <p>Every installation should have a way to verify success. Include commands that return expected output.</p> <p>3. Handle Alternatives</p> <p>Provide options for different package managers, operating systems, or configurations.</p> <p>4. Keep TODO Focused</p> <p>Each TODO item should be a clear, completable task.</p> <p>5. Use Code Blocks</p> <p>Always use fenced code blocks with language hints for commands.</p>"},{"location":"install-md-standard/#llmenergy-support","title":"llm.energy Support","text":"<p>llm.energy automatically:</p> <ul> <li>Detects install.md at <code>/install.md</code> and <code>/docs/install.md</code></li> <li>Extracts and includes it in your download bundle</li> <li>Validates the format and structure</li> <li>Generates install.md files with the wizard</li> </ul>"},{"location":"install-md-standard/#learn-more","title":"Learn More","text":"<ul> <li>installmd.org - Official specification</li> <li>GitHub: mintlify/install-md - Open source spec</li> <li>Mintlify Blog - Announcement post</li> </ul>"},{"location":"llms-txt-standard/","title":"The llms.txt Standard","text":"<p>Understanding the llms.txt standard for AI-readable documentation.</p>"},{"location":"llms-txt-standard/#what-is-llmstxt","title":"What is llms.txt?","text":"<p>The llms.txt standard is a convention for providing documentation in a format optimized for Large Language Models (LLMs).</p> <p>Similar to how <code>robots.txt</code> tells search engines how to crawl a site, <code>llms.txt</code> provides AI assistants with structured documentation.</p>"},{"location":"llms-txt-standard/#file-locations","title":"File Locations","text":"File Purpose <code>/llms.txt</code> Concise overview and key information <code>/llms-full.txt</code> Complete documentation"},{"location":"llms-txt-standard/#format","title":"Format","text":"<p>llms.txt files are plain markdown:</p> <pre><code># Project Name\n\n&gt; Brief description\n\n## Section 1\n\nContent...\n\n## Section 2\n\nContent...\n</code></pre>"},{"location":"llms-txt-standard/#why-it-matters","title":"Why It Matters","text":"<p>For AI Assistants:</p> <ul> <li>Structured content that's easy to parse</li> <li>No HTML/CSS/JS to filter out</li> <li>Complete documentation in one request</li> </ul> <p>For Developers:</p> <ul> <li>AI assistants give better answers about your product</li> <li>Reduces hallucination by providing accurate source material</li> <li>One source of truth for AI-consumed docs</li> </ul>"},{"location":"llms-txt-standard/#adoption","title":"Adoption","text":"<p>Sites with llms.txt support include:</p> <ul> <li>Anthropic</li> <li>Model Context Protocol</li> <li>Stripe</li> <li>Vercel</li> <li>Mintlify</li> </ul>"},{"location":"llms-txt-standard/#adding-llmstxt-to-your-site","title":"Adding llms.txt to Your Site","text":"<p>Option 1: Manual</p> <p>Create <code>/public/llms.txt</code> or <code>/public/llms-full.txt</code> with your documentation in markdown format.</p> <p>Option 2: Automated</p> <p>Documentation platforms like Mintlify automatically generate llms.txt from your docs.</p> <p>Best Practices:</p> <ul> <li>Include all essential documentation</li> <li>Use clear markdown headers</li> <li>Keep content up to date with your main docs</li> <li>Provide both concise (<code>llms.txt</code>) and full (<code>llms-full.txt</code>) versions</li> </ul>"},{"location":"llms-txt-standard/#learn-more","title":"Learn More","text":"<ul> <li>llmstxt.org - Official standard</li> <li>Anthropic's llms.txt - Example implementation</li> </ul>"},{"location":"overview/","title":"Overview","text":"<p>How llm.energy works.</p>"},{"location":"overview/#supported-standards","title":"Supported Standards","text":"<p>llm.energy supports two complementary standards for LLM-friendly documentation:</p>"},{"location":"overview/#llmstxt-standard","title":"llms.txt Standard","text":"<p>Many documentation sites provide their content in a machine-readable format at <code>/llms.txt</code> or <code>/llms-full.txt</code>. This format is designed for AI consumption.</p> Endpoint Description <code>/llms.txt</code> Concise version <code>/llms-full.txt</code> Complete documentation"},{"location":"overview/#installmd-standard","title":"install.md Standard","text":"<p>The install.md standard provides structured installation instructions that LLMs can execute autonomously.</p> Endpoint Description <code>/install.md</code> LLM-executable installation guide <code>/docs/install.md</code> Alternative location <p>install.md files include:</p> <ul> <li>OBJECTIVE: What the installation should achieve</li> <li>DONE WHEN: Success criteria</li> <li>TODO: Checklist of tasks</li> <li>Steps: Detailed instructions with code blocks</li> </ul>"},{"location":"overview/#how-it-works","title":"How It Works","text":"<pre><code>flowchart LR\n    A[Enter URL] --&gt; B[Fetch llms.txt]\n    B --&gt; C[Fetch install.md]\n    C --&gt; D[Parse Content]\n    D --&gt; E[Generate Files]\n    E --&gt; F[Download]</code></pre>"},{"location":"overview/#1-fetch","title":"1. Fetch","text":"<p>llm.energy fetches:</p> <ol> <li><code>{url}/llms-full.txt</code> (complete documentation)</li> <li>Falls back to <code>{url}/llms.txt</code> if full version unavailable</li> <li><code>{url}/install.md</code> (installation instructions)</li> <li>Also checks <code>/docs/install.md</code> and <code>docs.{domain}/install.md</code></li> </ol>"},{"location":"overview/#2-parse","title":"2. Parse","text":"<p>Content is split into sections based on markdown headers:</p> <ul> <li><code>##</code> headers become document boundaries</li> <li>Each section becomes its own <code>.md</code> file</li> <li>Filenames are generated from section titles</li> </ul>"},{"location":"overview/#3-generate","title":"3. Generate","text":"<p>Four types of output:</p> File Description <code>*.md</code> Individual section files <code>llms-full.md</code> Consolidated document with TOC <code>AGENT-GUIDE.md</code> Instructions for AI assistants <code>install.md</code> Installation instructions (if available)"},{"location":"overview/#supported-sites","title":"Supported Sites","text":"<p>Any site that provides <code>/llms.txt</code> or <code>/llms-full.txt</code> will work:</p> <ul> <li>modelcontextprotocol.io</li> <li>docs.anthropic.com</li> <li>docs.stripe.com</li> <li>And many more...</li> </ul>"},{"location":"self-hosting/","title":"Self Hosting","text":"<p>Deploy llm.energy on your own infrastructure.</p>"},{"location":"self-hosting/#requirements","title":"Requirements","text":"<ul> <li>Node.js 18+</li> <li>pnpm (recommended)</li> </ul>"},{"location":"self-hosting/#clone-install","title":"Clone &amp; Install","text":"<pre><code>git clone https://github.com/nirholas/extract-llms-docs.git\ncd extract-llms-docs\npnpm install\n</code></pre>"},{"location":"self-hosting/#development","title":"Development","text":"<pre><code>pnpm dev\n</code></pre> <p>Open http://localhost:3001</p>"},{"location":"self-hosting/#production-build","title":"Production Build","text":"<pre><code>pnpm build\npnpm start\n</code></pre>"},{"location":"self-hosting/#deploy-to-vercel","title":"Deploy to Vercel","text":"<p>Or via CLI:</p> <pre><code>vercel --prod\n</code></pre>"},{"location":"self-hosting/#environment-variables","title":"Environment Variables","text":"Variable Description Default <code>PORT</code> Server port <code>3001</code>"},{"location":"self-hosting/#mcp-server","title":"MCP Server","text":"<p>To run the MCP server locally:</p> <pre><code>cd llms-forge/mcp-server\npnpm install\npnpm build\npnpm start\n</code></pre>"},{"location":"mcp-server/installation/","title":"Installation","text":"<p>Set up the llm.energy MCP server for AI assistants.</p>"},{"location":"mcp-server/installation/#what-is-mcp","title":"What is MCP?","text":"<p>The Model Context Protocol (MCP) allows AI assistants to interact with external tools and data sources.</p>"},{"location":"mcp-server/installation/#quick-setup","title":"Quick Setup","text":"npx (Recommended)Global Install <p>No installation required:</p> <pre><code>{\n  \"mcpServers\": {\n    \"llm-energy\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@llm-energy/mcp-server\"]\n    }\n  }\n}\n</code></pre> <pre><code>npm install -g @llm-energy/mcp-server\n</code></pre> <pre><code>{\n  \"mcpServers\": {\n    \"llm-energy\": {\n      \"command\": \"llm-energy-mcp\"\n    }\n  }\n}\n</code></pre>"},{"location":"mcp-server/installation/#configuration","title":"Configuration","text":"Claude DesktopVS Code / Cursor <p>Add to your Claude Desktop config:</p> OS Path macOS <code>~/Library/Application Support/Claude/claude_desktop_config.json</code> Windows <code>%APPDATA%\\Claude\\claude_desktop_config.json</code> <p>Add to workspace settings or MCP configuration:</p> <pre><code>{\n  \"llm-energy\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@llm-energy/mcp-server\"]\n  }\n}\n</code></pre>"},{"location":"mcp-server/installation/#verify-installation","title":"Verify Installation","text":"<p>After configuration, restart your AI client. Ask:</p> <p>\"What tools does llm-energy provide?\"</p> <p>The assistant should list the available extraction tools.</p>"},{"location":"mcp-server/tools/","title":"Tools Reference","text":"<p>Available MCP tools for llm.energy.</p>"},{"location":"mcp-server/tools/#extract_documentation","title":"extract_documentation","text":"<p>Extract and parse documentation from a website with llms.txt support.</p> <p>Parameters</p> Name Type Required Description <code>url</code> string Yes Documentation site URL <p>Example</p> <pre><code>{\n  \"url\": \"docs.anthropic.com\"\n}\n</code></pre> <p>Output</p> <ul> <li>Extraction summary</li> <li>List of parsed documents with token counts</li> <li>Statistics (processing time, document count, total tokens)</li> </ul> <p>Caching</p> <p>After extraction, documentation is cached for quick access via other tools.</p>"},{"location":"mcp-server/tools/#fetch_llms_txt","title":"fetch_llms_txt","text":"<p>Fetch raw llms.txt content without parsing.</p> <p>Parameters</p> Name Type Required Description <code>url</code> string Yes Documentation site URL <code>full</code> boolean No Fetch llms-full.txt instead (default: true) <p>Example</p> <pre><code>{\n  \"url\": \"modelcontextprotocol.io\",\n  \"full\": true\n}\n</code></pre>"},{"location":"mcp-server/tools/#get_document","title":"get_document","text":"<p>Retrieve a specific document from the cache.</p> <p>Parameters</p> Name Type Required Description <code>name</code> string Yes Document filename <p>Example</p> <pre><code>{\n  \"name\": \"getting-started.md\"\n}\n</code></pre>"},{"location":"mcp-server/tools/#list_documents","title":"list_documents","text":"<p>List all cached documents.</p> <p>Output</p> <p>Array of document names with metadata:</p> <ul> <li>Filename</li> <li>Token count</li> <li>Section title</li> </ul>"}]}